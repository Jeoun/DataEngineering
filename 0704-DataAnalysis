20190704-StartTheFirst2



[sql file scp]
authors-23-04-2019-02-34-beta.sql.zip
posts23-04-2019-02-44.sql.zip

scp  -i ./SKT.pem ./authors-23-04-2019-02-34-beta.sql.zip centos@52.78.148.50:/home/centos
scp  -i ./SKT.pem ./posts23-04-2019-02-44.sql.zip centos@52.78.148.50:/home/centos


[root@cm centos]# ls -al *.sql
-rwxrwxrwx 1 root root   892780 Apr 23 02:34 authors-23-04-2019-02-34-beta.sql
-rwxrwxrwx 1 root root 52680682 Apr 23 02:44 posts23-04-2019-02-44.sql

mysql -u root -p
CREATE DATABASE test DEFAULT CHARACTER SET utf8  DEFAULT COLLATE utf8_general_ci;
GRANT ALL ON test.* TO 'training'@'%' IDENTIFIED BY 'Hadoop123!';
flush privileges;

mysql -u training -p
source authors-23-04-2019-02-34-beta.sql
source posts23-04-2019-02-44.sql




[spark2 install]
https://www.cloudera.com/documentation/spark2/latest/topics/spark2.html
CDS Powered by Apache Spark Overview
 CDS(custom service descriptor)



1. To download the CDS Powered by Apache Spark service descriptor, in the Version Information table in CDS Versions Available for Download, click the service descriptor link for the version you want to install.

Available CDS Versions
Version	        Custom Service Descriptor	        Parcel Repository
2.4 Release 2	SPARK2_ON_YARN-2.4.0.cloudera2.jar	http://archive.cloudera.com/spark2/parcels/2.4.0.cloudera2/


2. Log on to the Cloudera Manager Server host, and copy the CDS Powered by Apache Spark service descriptor in the location configured for service descriptor files.

scp  -i ./SKT.pem ./SPARK2_ON_YARN-2.4.0.cloudera2.jar centos@52.78.148.50:/home/centos


3. Set the file ownership of the service descriptor to cloudera-scm:cloudera-scm with permission 644.

cp /home/centos/SPARK2_ON_YARN-2.4.0.cloudera2.jar /opt/cloudera/csd/
chown cloudera-scm:cloudera-scm /opt/cloudera/csd/SPARK2_ON_YARN-2.4.0.cloudera2.jar
chmod 644 /opt/cloudera/csd/SPARK2_ON_YARN-2.4.0.cloudera2.jar



4. Restart the Cloudera Manager Server with the following command:
systemctl restart cloudera-scm-server


5. Download the CDS Powered by Apache Spark parcel, distribute the parcel to the hosts in your cluster, and activate the parcel. See Managing Parcels.

cm 메뉴 옆에 있는 Parcel menu 에서 spark2 에 대한 설정 진행 



------------------------------------------------------


Let’s test out our cluster 

1. Create user “training” in linux and in hdfs. 

training/Hadoop123!

useradd training -G hadoop 
passwd training 
hdfs dfs -mkdir /user/training
hdfs dfs -chown training:hadoop /user/training


2. 
mysql -u root -p
CREATE DATABASE test DEFAULT CHARACTER SET utf8  DEFAULT COLLATE utf8_general_ci;
GRANT ALL ON *.* TO 'training'@'%' IDENTIFIED BY 'Admin123!';

mysql -u training -p
source authors-23-04-2019-02-34-beta.sql
source posts23-04-2019-02-44.sql


3. Extract tables authors and posts from the database and create Hive tables. 


#check 
beeline -u jdbc:hive2://cm.skplanet.com:10000 -n hive

#scoop import authors, posts

sqoop import --connect jdbc:mysql://cm:3306/test --username training --password Hadoop123! --table authors  --driver com.mysql.jdbc.Driver --target-dir /user/training/authors --hive-import --hive-table test.authors 
sqoop import --connect jdbc:mysql://cm:3306/test --username training --password Hadoop123! --table posts  --driver com.mysql.jdbc.Driver --target-dir /user/training/posts --hive-import --hive-table test.posts



4. Create and run a Hive/Impala query. 
From the query, generate the results dataset that you will use in the next step to export in MySQL. 
a. Create a query that counts the number of posts each author has created. 
i. The id column in authors matches the author_id key in posts. 
b. The output of the query should provide the following informa;on: 



http://ec2-15-164-136-136.ap-northeast-2.compute.amazonaws.com:8889/hue/editor?editor=3&type=hive

hdfs dfs -mkdir /user/training/yelp
hdfs dfs -mkdir /user/training/yelp/business 
hdfs dfs -mkdir /user/training/yelp/review 
hdfs dfs -mkdir /user/training/yelp/users 
hdfs dfs -mkdir /user/training/yelp/tip 
hdfs dfs -mkdir /user/training/yelp/checkin 

cd /home/training/dataset
hdfs dfs -put business.json /user/training/yelp/business/
hdfs dfs -put checkin.json /user/training/yelp/checkin/
hdfs dfs -put review.json /user/training/yelp/review/
hdfs dfs -put tip.json /user/training/yelp/tip/
hdfs dfs -put user.json /user/training/yelp/users/
#hdfs dfs -put photos.json /user/training/yelp/photos/


Step 3: Adding RCONGUI JSON SerDe 
(this step may not be necessary with current hive version) 

wget -O json-serde-1.3.8-jar-with-dependencies.jar  http://www.congiu.net/hive-json-serde/1.3.8/cdh5/json-serde-1.3.8-jar-with-dependencies.jar

wget -O json-udf-1.3.8-jar-with-dependencies.jar http://www.congiu.net/hive-json-serde/1.3.8/cdh5/json-udf-1.3.8-jar-with-dependencies.jar

ADD JAR json-serde-1.3.8-jar-with-dependencies.jar; 
ADD JAR json-udf-1.3.8-jar-with-dependencies.jar; 

set hive.cli.print.header=true;


CREATE EXTERNAL TABLE business4 (
address string,
business_id string,
categories array<string>,
city string,
hours struct<friday:string, monday:string, saturday:string, sunday:string, thursday:string,
tuesday:string, wednesday:string>,
is_open int,
latitude double, 
longitude double,
name string,
neighborhood string,
postal_code string,
review_count int,
stars double,
state string,
Attributes struct<
Accepts_Insurance:boolean,
Ages_Allowed:string,
Alcohol:string,
Bike_Parking:boolean,
Business_Accepts_Bitcoin:boolean,
Business_Accepts_Credit_Cards:boolean,
By_Appointment_Only:boolean,
Byob:boolean,
BYOB_Corkage:string,
Caters:boolean,
Coat_Check:boolean,
Corkage:boolean,
Dogs_Allowed:boolean, 
Drive_Thru:boolean,
Good_For_Dancing:boolean,
Good_For_Kids:boolean,
Happy_Hour:boolean,
Has_TV:boolean,
Noise_Level:string,
Open24Hours:boolean,
Outdoor_Seating:boolean,
Restaurants_Attire:string,
Restaurants_Counter_Service:boolean, 
Restaurants_Delivery:boolean,
Restaurants_Good_For_Groups:boolean,
Restaurants_Reservations:boolean,
Restaurants_Table_Service:boolean,
Restaurants_Take_Out:boolean,
Smoking:string,
WheelchairAccessible:boolean, 
WiFi:string,
Ambience:struct<
Casual:boolean,
Classy:boolean,
Divey:boolean,
Hipster:boolean,
Intimate:boolean,
Romantic:boolean,
Touristy:boolean,
Trendy:boolean,
Upscale:boolean>,
BestNights:struct<
Friday1:boolean,
Monday1:boolean,
Saturday1:boolean, 
Sunday1:boolean,
Thursday1:boolean,
Tuesday1:boolean,
Wednesday1:boolean>,
BusinessParking:struct<
Garage:boolean,
Lot:boolean,
Street:boolean,
Valet:boolean,
Validated:boolean>,
DietaryRestrictions:struct<
Dairy_Free:boolean,
Gluten_Free:boolean,
Halal:boolean,
Kosher:boolean,
Soy_Free:boolean,
Vegan:boolean,
Vegetarian:boolean>,
GoodForMeal:struct<
Breakfast:boolean,
Brunch:boolean,
Dessert:boolean, 
Dinner:boolean,
Latenight:boolean,
Lunch:boolean>,
HairSpecializesIn:struct<
Africanamerican:boolean,
Asian:boolean,
Coloring:boolean,
Curly:boolean,
Extensions:boolean,
Kids:boolean,
Perms:boolean,
Straightperms:boolean>,
Music:struct<
BackgroundMusic:boolean,
Dj:boolean,
Jukebox:boolean,
Karaoke:boolean,
Live:boolean, 
NoMusic:boolean,
Video:boolean>,
restaurantspricerange2:int>)
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
STORED AS TEXTFILE
LOCATION '/user/training/yelp/business'; 

ADD JAR json-serde-1.3.8-jar-with-dependencies.jar; 
ADD JAR json-udf-1.3.8-jar-with-dependencies.jar; 

set hive.cli.print.header=true;


use test;
SELECT count(business_id) FROM business4;
SELECT * FROM business4;
DESCRIBE business4; 

CREATE TABLE biz4 
STORED AS PARQUET 
AS SELECT * FROM business4;


xxxxxxx
CREATE ABLE explode 
....


SELECT * FROM exploded LIMIT 1;



SELECT count (DISTINCT business_id) number_businesses FROM exploded;


SELECT count (business_id) number_restaurants FROM exploded 
WHERE cat_exploded="Restaurants"; 



TABLE 3: Create RESTAURANTS table 
CREATE TABLE restaurants 
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' 
STORED AS TEXTFILE 
LOCATION '/user/training/yelp/business/restaurants' 
AS 
SELECT * FROM exploded WHERE cat_exploded="Restaurants"; 


SELECT name, review_count, stars, cat_exploded category FROM restaurants LIMIT 5;



SELECT name, attributes.ambience.romantic FROM restaurants LIMIT 5;
SELECT name, state, city, attributes.ambience.romantic romantic FROM restaurants 
WHERE attributes.ambience.romantic = true LIMIT 10; 



II. Create Review Table
Table 4: Create Review Table 

CREATE EXTERNAL TABLE review ( 
business_id string, 
cool int, 
review_date string, 
funny int, 
review_id string, 
stars int, 
text string, 
useful int, 
user_id string) 
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe' 
STORED AS TEXTFILE 
LOCATION '/user/training/yelp/review'; 

SELECT * FROM review LIMIT 1;
SELECT count(*) FROM review;


Table 5: Create Review_Filtered Table
xxx

SELECT count(*) FROM review_filtered;



III. Create Users Table 
Table 6: Create Users Table 
CREATE XTERNAL TABLE user (
...


SELECT count(distinct user_id) FROM users;




Table 7: Create Elite Users Table 
CREATE TABLE elite_users 



IV. Create Tip Table 
Table 8: Create Tip Table 
CREATE EXTERNAL TABLE tip ( 

...



Step 4: Create tables and queries using HiveQL and visualize in Hue (or Tableau if you want to try) 






-------------------------------------------------
